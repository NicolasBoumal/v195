---
title: Multiclass Online Learning and Uniform Convergence
abstract: We study multiclass classification in the agnostic adversarial online learning
  setting. As our main result, we prove that any multiclass concept class is agnostically
  learnable if and only if its Littlestone dimension is finite. This solves an open
  problem studied by Daniely, Sabato, Ben-David, and Shalev-Shwartz (2011,2015) who
  handled the case when the number of classes (or labels) is bounded. We also prove
  a separation between online learnability and online uniform convergence by exhibiting
  an easy-to-learn class whose sequential Rademacher complexity is unbounded.Our learning
  algorithm uses the multiplicative weights algorithm, with a set of experts defined
  by executions of the Standard Optimal Algorithm on subsequences of size Littlestone
  dimension. We argue that the best expert has regret at most Littlestone dimension
  relative to the best concept in the class.  This differs from the well-known covering
  technique of Ben-David, Pal, and Shalev-Shwartz (2009) for binary classification,
  where the best expert has regret zero.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hanneke23b
month: 0
tex_title: Multiclass Online Learning and Uniform Convergence
firstpage: 5682
lastpage: 5696
page: 5682-5696
order: 5682
cycles: false
bibtex_author: Hanneke, Steve and Moran, Shay and Raman, Vinod and Subedi, Unique
  and Tewari, Ambuj
author:
- given: Steve
  family: Hanneke
- given: Shay
  family: Moran
- given: Vinod
  family: Raman
- given: Unique
  family: Subedi
- given: Ambuj
  family: Tewari
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/hanneke23b/hanneke23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
