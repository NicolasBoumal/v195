---
title: Quasi-Newton Steps for Efficient Online Exp-Concave Optimization
abstract: The aim of this paper is to design computationally-efficient and optimal
  algorithms for the online and stochastic exp-concave optimization settings. Typical
  algorithms for these settings, such as the Online Newton Step (ONS), can guarantee
  a $O(d\ln T)$ bound on their regret after $T$ rounds, where $d$ is the dimension
  of the feasible set. However, such algorithms perform so-called \emph{generalized
  projections} whenever their iterates step outside the feasible set. Such generalized
  projections require $\Omega(d^3)$ arithmetic operations even for simple sets such
  a Euclidean ball, making the total runtime of ONS of order $d^3 T$ after $T$ rounds,
  in the worst-case. In this paper, we side-step generalized projections by using
  a self-concordant barrier as a regularizer to compute the Newton steps. This ensures
  that the iterates are always within the feasible set without requiring projections.
  This approach still requires the computation of the inverse of the Hessian of the
  barrier at every step. However, using stability properties of the Newton iterates,
  we show that the inverse of the Hessians can be efficiently approximated via Taylor
  expansions for most rounds, resulting in a $\widetilde O(d^2 T +d^\omega \sqrt{T})$
  total computational complexity, where $\omega\in(2,3]$ is the exponent of matrix
  multiplication. In the stochastic setting, we show that this translates into a $\widetilde
  O(d^3/\varepsilon)$ computational complexity for finding an $\varepsilon$-optimal
  point, answering an open question by Koren 2013. We first prove these new results
  for the simple case where the feasible set is a Euclidean ball. Then, to move to
  general convex sets, we use a reduction to Online Convex Optimization over the Euclidean
  ball. Our final algorithm for general convex sets can be viewed as a more computationally-efficient
  version of ONS.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: mhammedi23a
month: 0
tex_title: Quasi-Newton Steps for Efficient Online Exp-Concave Optimization
firstpage: 1
lastpage: 31
page: 1-31
order: 1
cycles: false
bibtex_author: Mhammedi, Zakaria and Gatmiry, Khashayar
author:
- given: Zakaria
  family: Mhammedi
- given: Khashayar
  family: Gatmiry
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/mhammedi23a/mhammedi23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
