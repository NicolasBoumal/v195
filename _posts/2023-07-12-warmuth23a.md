---
title: 'Open Problem: Learning sparse linear concepts by priming the features'
abstract: Sparse linear problems can be learned well with online multiplicative updates.
  The question is weather there are closed form updates based on the past examples
  that can sample efficiently learn such sparse linear problems as well?We show experimentally
  that this can be achieved by applying linear least squares, then “priming” the ith
  features of the past instances by multiplying them by the ith linear least squares
  weight, and finally applying linear least squares a second time. However it is an
  open problem whether such priming methods have provably good regret bounds when
  applied online?
section: Open Problems
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: warmuth23a
month: 0
tex_title: 'Open Problem: Learning sparse linear concepts by priming the features'
firstpage: 5937
lastpage: 5942
page: 5937-5942
order: 5937
cycles: false
bibtex_author: Warmuth, Manfred K. and Amid, Ehsan
author:
- given: Manfred K.
  family: Warmuth
- given: Ehsan
  family: Amid
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/warmuth23a/warmuth23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
