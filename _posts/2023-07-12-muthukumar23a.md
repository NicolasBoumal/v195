---
title: Sparsity-aware generalization theory for deep neural networks
abstract: Deep artificial neural networks achieve surprising generalization abilities
  that remain poorly understood. In this paper, we present a new approach to analyzing
  generalization for deep feed-forward ReLU networks that takes advantage of the degree
  of sparsity that is achieved in the hidden layer activations. By developing a framework
  that accounts for this reduced effective model size for each input sample, we are
  able to show fundamental trade-offs between sparsity and generalization. Importantly,
  our results make no strong assumptions about the degree of sparsity achieved by
  the model, and it improves over recent norm-based approaches. We  illustrate our
  results numerically, demonstrating non-vacuous bounds when coupled with data-dependent
  priors even in over-parametrized settings.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: muthukumar23a
month: 0
tex_title: Sparsity-aware generalization theory for deep neural networks
firstpage: 5311
lastpage: 5342
page: 5311-5342
order: 5311
cycles: false
bibtex_author: Muthukumar, Ramchandran and Sulam, Jeremias
author:
- given: Ramchandran
  family: Muthukumar
- given: Jeremias
  family: Sulam
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/muthukumar23a/muthukumar23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
