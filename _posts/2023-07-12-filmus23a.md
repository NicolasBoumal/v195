---
title: Optimal Prediction Using Expert Advice and Randomized Littlestone Dimension
abstract: 'A classical result in online learning characterizes the optimal mistake
  bound achievable by deterministic learners using the Littlestone dimension (Littlestone
  ’88).We prove an analogous result for randomized learners: we show that the optimal
  expected mistake bound in learning a class $\mathcal{H}$ equals its randomized Littlestone
  dimension, which we define as follows: it is the largest $d$ for which there exists
  a tree shattered by $\mathcal{H}$ whose average depth is $2d$.We further study optimal
  mistake bounds in the agnostic case, as a function of the number of mistakes made
  by the best function in $\mathcal{H}$, denoted by $k$. Towards this end we introduce
  the $k$-Littlestone dimension and its randomized variant, and use them to characterize
  the optimal deterministic and randomized mistake bounds.Quantitatively, we show
  that the optimal randomized mistake bound for learning a class with Littlestone
  dimension $d$ is $k + \Theta (\sqrt{k d} + d )$ (equivalently, the optimal regret
  is $\Theta(\sqrt{kd} + d$). This also implies an optimal deterministic mistake bound
  of $2k + O (\sqrt{k d} + d )$, thus resolving an open question which was studied
  by Auer and Long [’99]. As an application of our theory, we revisit the classical
  problem of prediction using expert advice: about 30 years ago Cesa-Bianchi, Freund,
  Haussler, Helmbold, Schapire and Warmuth studied prediction using expert advice,
  provided that the best among the $n$ experts makes at most $k$ mistakes, and asked
  what are the optimal mistake bounds (as a function of $n$ and $k$). Cesa-Bianchi,
  Freund, Helmbold, and Warmuth [’93, ’96] provided a nearly optimal bound for deterministic
  learners, and left the randomized case as an open problem. We resolve this question
  by providing an optimal learning rule in the randomized case, and showing that its
  expected mistake bound equals half of the deterministic bound, up to negligible
  additive terms. This improves upon previous works by Cesa-Bianchi, Freund, Haussler,
  Helmbold, Schapire and Warmuth [’93, ’97], by Abernethy, Langford, and Warmuth [’06],
  and by Brânzei and Peres [’19], which handled the regimes $k \ll \log n$ or $k\gg
  \log n$. In contrast, our result applies to all pairs $n,k$, and does so via a unified
  analysis using the randomized Littlestone dimension.In our proofs we develop and
  use optimal learning rules, which can be seen as natural variants of the Standard
  Optimal Algorithm ($\mathsf{SOA}$) of Littlestone: a weighted variant in the agnostic
  case, and a probabilistic variant in the randomized case. We conclude the paper
  with suggested directions for future research and open questions.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: filmus23a
month: 0
tex_title: Optimal Prediction Using Expert Advice and Randomized Littlestone Dimension
firstpage: 773
lastpage: 836
page: 773-836
order: 773
cycles: false
bibtex_author: Filmus, Yuval and Hanneke, Steve and Mehalel, Idan and Moran, Shay
author:
- given: Yuval
  family: Filmus
- given: Steve
  family: Hanneke
- given: Idan
  family: Mehalel
- given: Shay
  family: Moran
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/filmus23a/filmus23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
