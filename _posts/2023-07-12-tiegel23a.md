---
title: Hardness of Agnostically Learning Halfspaces from Worst-Case Lattice Problems
abstract: We show hardness of improperly learning halfspaces in the agnostic model,
  both in the distribution-independent as well as the distribution-specific setting,
  based on the assumption that worst-case lattice problems, e.g., approximating shortest
  vectors within polynomial factors, are hard.In particular, we show that under this
  assumption there is no efficient algorithm that outputs any binary hypothesis, not
  necessarily a halfspace, achieving misclassfication error better than $\frac 1 2
  - \gamma$ even if the optimal misclassification error is as small is as small as
  $\delta$.Here, $\gamma$ can be smaller than the inverse of any polynomial in the
  dimension and $\delta$ as small as $\exp(-\Omega(\log^{1-c}(d)))$, where $0 < c
  < 1$ is an arbitrary constant and $d$ is the dimension.For the distribution-specific
  setting, we show that if the marginal distribution is standard Gaussian, for any
  $\beta > 0$ learning halfspaces up to error $\OPT_\LTF + \varepsilon$ takes time
  at least $d^{\tilde{\Omega}(1/\varepsilon^{2-\beta})}$ under the same hardness assumptions.Similarly,
  we show that learning degree-$\ell$ polynomial threshold functions up to error $\OPT_{\PTF_\ell}
  + \e$ takes time at least $d^{\tilde{\Omega}(\ell^{2-\beta}/\varepsilon^{4-2\beta})}$.$\OPT_\LTF$
  and $\OPT_{\PTF_\ell}$ denote the best error achievable by any halfspace or polynomial
  threshold function, respectively.Our lower bounds qualitively match algorithmic
  guarantees and (nearly) recover known lower bounds based on non-worst-case assumptions.Previously,
  such hardness results [D16, DKPZ21] were based on average-case complexity assumptions,
  specifically, variants of Feigeâ€™s random 3SAT hypothesis, or restricted to the statistical
  query model.Our work gives the first hardness results basing these fundamental learning
  problems on well-understood worst-case complexity assumption.It is inspired by a
  sequence of recent works showing hardness of learning well-separated Gaussian mixtures
  based on worst-case lattice problems.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: tiegel23a
month: 0
tex_title: Hardness of Agnostically Learning Halfspaces from Worst-Case Lattice Problems
firstpage: 3029
lastpage: 3064
page: 3029-3064
order: 3029
cycles: false
bibtex_author: Tiegel, Stefan
author:
- given: Stefan
  family: Tiegel
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/tiegel23a/tiegel23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
