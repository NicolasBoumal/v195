---
title: 'VO$Q$L: Towards Optimal Regret in Model-free RL with Nonlinear Function Approximation'
abstract: We study time-inhomogeneous episodic reinforcement learning (RL) under general
  function approximation and sparse rewards. We design a new algorithm, Variance-weighted
  Optimistic $Q$-Learning (VO$Q$L), based on  $Q$-learning and bound its regret assuming
  closure under Bellman backups, and bounded Eluder dimension for the regression function
  class. As a special case, VO$Q$L achieves $\widetilde{O}(d\sqrt{TH}+d^6H^{5})$ regret
  over $T$ episodes for a horizon $H$ MDP under ($d$-dimensional) linear function
  approximation, which is asymptotically optimal. Our algorithm incorporates weighted
  regression-based upper and lower bounds on the optimal value function to obtain
  this improved regret. The algorithm is computationally efficient given a regression
  oracle over the function class, making this the first computationally tractable
  and statistically optimal approach for linear MDPs.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: agarwal23a
month: 0
tex_title: 'VO$Q$L: Towards Optimal Regret in Model-free RL with Nonlinear Function
  Approximation'
firstpage: 987
lastpage: 1063
page: 987-1063
order: 987
cycles: false
bibtex_author: Agarwal, Alekh and Jin, Yujia and Zhang, Tong
author:
- given: Alekh
  family: Agarwal
- given: Yujia
  family: Jin
- given: Tong
  family: Zhang
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/agarwal23a/agarwal23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
