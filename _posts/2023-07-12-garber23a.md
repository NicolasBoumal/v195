---
title: Projection-free Online Exp-concave Optimization
abstract: 'We consider the setting of online convex optimization (OCO) with \textit{exp-concave}
  losses. The best regret bound known for this setting is $O(n\log{}T)$, where $n$
  is the dimension and $T$ is the number of prediction rounds (treating all other
  quantities as constants and assuming $T$ is sufficiently large), and is attainable
  via the well-known Online Newton Step algorithm (ONS). However, ONS requires on
  each iteration to compute a projection (according to some matrix-induced norm) onto
  the feasible convex set, which is often computationally prohibitive in high-dimensional
  settings and when the feasible set admits a non-trivial structure. In this work
  we consider projection-free online algorithms for  exp-concave and smooth losses,
  where by projection-free we refer to algorithms that rely only on the availability
  of a linear optimization oracle (LOO) for the feasible set, which in many applications
  of interest admits much more efficient implementations than a projection oracle.
  We present an LOO-based ONS-style algorithm, which using overall $O(T)$ calls to
  a LOO, guarantees in worst case regret bounded by $\widetilde{O}(n^{2/3}T^{2/3})$
  (ignoring all quantities except for $n,T$). However, our algorithm is most interesting
  in an important and plausible low-dimensional data scenario: if the gradients  (approximately)
  span a subspace of dimension at most $\rho$, $\rho << n$, the regret bound improves
  to $\widetilde{O}(\rho^{2/3}T^{2/3})$, and by applying standard deterministic sketching
  techniques, both the space and average additional per-iteration runtime requirements
  are only $O(\rho{}n)$ (instead of $O(n^2)$). This  improves upon recently proposed
  LOO-based algorithms for OCO which, while having the same state-of-the-art dependence
  on the horizon $T$, suffer from regret/oracle complexity that scales with $\sqrt{n}$
  or worse.'
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: garber23a
month: 0
tex_title: Projection-free Online Exp-concave Optimization
firstpage: 1259
lastpage: 1284
page: 1259-1284
order: 1259
cycles: false
bibtex_author: Garber, Dan and Kretzu, Ben
author:
- given: Dan
  family: Garber
- given: Ben
  family: Kretzu
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/garber23a/garber23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
