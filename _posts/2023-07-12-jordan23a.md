---
title: Deterministic Nonsmooth Nonconvex Optimization
abstract: We study the complexity of optimizing nonsmooth nonconvex Lipschitz functions
  by producing $(\delta,\epsilon)$-Goldstein stationary points. Several recent works
  have presented randomized algorithms that produce such points using $\widetilde{O}(\delta^{-1}\epsilon^{-3})$
  first-order oracle calls, independent of the dimension $d$.  It has been an open
  problem as to whether a similar result can be obtained via a deterministic algorithm.
  We resolve this open problem, showing that randomization is necessary to obtain
  a dimension-free rate. In particular, we prove a lower bound of $\Omega(d)$ for
  any deterministic algorithm. Moreover, we show that unlike smooth or convex optimization,
  access to function values is required for any deterministic algorithm to halt within
  any finite time horizon.On the other hand, we prove that if the function is even
  slightly smooth, then the dimension-free rate of $\widetilde{O}(\delta^{-1}\epsilon^{-3})$
  can be obtained by a deterministic algorithm with merely a logarithmic dependence
  on the smoothness parameter. Motivated by these findings, we turn to study the complexity
  of deterministically smoothing Lipschitz functions. Though there are well-known
  efficient black-box randomized smoothings, we start by showing that no such deterministic
  procedure can smooth functions in a meaningful manner (suitably defined), resolving
  an open question in the literature. We then bypass this impossibility result for
  the structured case of ReLU neural networks. To that end, in a practical “white-box”
  setting in which the optimizer is granted access to the network’s architecture,
  we propose a simple, dimension-free, deterministic smoothing of ReLU networks that
  provably preserves $(\delta,\epsilon)$-Goldstein stationary points. Our method applies
  to a variety of architectures of arbitrary depth, including ResNets and ConvNets.Combined
  with our algorithm for slightly-smooth functions, this yields the first deterministic,
  dimension-free algorithm for optimizing ReLU networks, circumventing our lower bound.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: jordan23a
month: 0
tex_title: Deterministic Nonsmooth Nonconvex Optimization
firstpage: 1
lastpage: 28
page: 1-28
order: 1
cycles: false
bibtex_author: Jordan, Michael and Kornowski, Guy and Lin, Tianyi and Shamir, Ohad
  and Zampetakis, Manolis
author:
- given: Michael
  family: Jordan
- given: Guy
  family: Kornowski
- given: Tianyi
  family: Lin
- given: Ohad
  family: Shamir
- given: Manolis
  family: Zampetakis
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/jordan23a/jordan23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
