---
title: 'Multitask Learning via Shared Features: Algorithms and Hardness'
abstract: We investigate the computational efficiency of multitask learning of Boolean
  functions over the $d$-dimensional hypercube, that are related by means of a feature
  representation of size $k\ll d$ shared across all tasks. We present a polynomial
  time multitask learning algorithm for the concept class of halfspaces with margin
  $\gamma$, which is based on a simultaneous boosting technique and requires only
  $\mathrm{poly}(k/\gamma)$ samples-per-task and $\mathrm{poly}(k\log(d)/\gamma)$
  samples in total. In addition, we prove a computational separation, showing that
  assuming there exists a concept class that cannot be learned in the attribute-efficient
  model, we can construct another concept class such that can be learned in the attribute-efficient
  model, but cannot be multitask learned efficientlyâ€”multitask learning this concept
  class either requires super-polynomial time complexity or a much larger total number
  of samples.
section: Original Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: bairaktari23a
month: 0
tex_title: 'Multitask Learning via Shared Features: Algorithms and Hardness'
firstpage: 747
lastpage: 772
page: 747-772
order: 747
cycles: false
bibtex_author: Bairaktari, Konstantina and Blanc, Guy and Tan, Li-Yang and Ullman,
  Jonathan and Zakynthinou, Lydia
author:
- given: Konstantina
  family: Bairaktari
- given: Guy
  family: Blanc
- given: Li-Yang
  family: Tan
- given: Jonathan
  family: Ullman
- given: Lydia
  family: Zakynthinou
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/bairaktari23a/bairaktari23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
