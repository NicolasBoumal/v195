---
title: A Blackbox Approach to Best of Both Worlds in Bandits and Beyond
abstract: Best-of-both-worlds algorithms for online learning which achieve near-optimal
  regret in both the adversarial and the stochastic regimes have received growing
  attention recently. Existing techniques often require careful adaptation to every
  new problem setup, including specialized potentials and careful tuning of algorithm
  parameters. Yet, in domains such as linear bandits, it is still unknown if there
  exists an algorithm that can obtain $O(\log(T))$ regret in the stochastic regime
  and $\tilde{O}(\sqrt{T})$ regret in the adversarial regime. In this work, we resolve
  this question positively and present a generally applicable reduction from best
  of both worlds to a wide family of follow-the-regularized-leader (FTRL) algorithms.
  We showcase the capability of this reduction by transforming existing algorithms
  that only achieve worst-case guarantees into new best-of-both-worlds algorithms
  in the setting of contextual bandits, graph bandits and tabular Markov decision
  processes.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: dann23a
month: 0
tex_title: A Blackbox Approach to Best of Both Worlds in Bandits and Beyond
firstpage: 5503
lastpage: 5570
page: 5503-5570
order: 5503
cycles: false
bibtex_author: Dann, Chris and Wei, Chen-Yu and Zimmert, Julian
author:
- given: Chris
  family: Dann
- given: Chen-Yu
  family: Wei
- given: Julian
  family: Zimmert
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/dann23a/dann23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
