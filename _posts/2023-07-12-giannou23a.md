---
title: The Expressive Power of Tuning Only the Normalization Layers
abstract: Feature normalization transforms such as Batch and Layer-Normalization have
  become indispensable ingredients of state-of-the-art deep neural networks. Recent
  studies on fine-tuning large pretrained models indicate that just tuning the parameters
  of these affine transforms can achieve high accuracy for downstream tasks. These
  findings open the questions about the expressive power of tuning  the normalization
  layers of frozen networks. In this work, we take the first step towards this question
  and show that for random ReLU networks, finetuning only its normalization layers
  can reconstruct  any target network that is $O(\sqrt{\text{width}})$ times smaller.
  We show that this holds even for randomly sparsified networks, under sufficient
  overparameterization, in agreement with prior empirical work.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: giannou23a
month: 0
tex_title: The Expressive Power of Tuning Only the Normalization Layers
firstpage: 1
lastpage: 2
page: 1-2
order: 1
cycles: false
bibtex_author: Giannou, Angeliki and Rajput, Shashank and Papailiopoulos, Dimitris
author:
- given: Angeliki
  family: Giannou
- given: Shashank
  family: Rajput
- given: Dimitris
  family: Papailiopoulos
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/giannou23a/giannou23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
