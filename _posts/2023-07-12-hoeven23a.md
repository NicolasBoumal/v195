---
title: A Unified Analysis of Nonstochastic Delayed Feedback for Combinatorial Semi-Bandits,
  Linear Bandits, and MDPs
abstract: We derive a new analysis of Follow The Regularized Leader (FTRL) for online
  learning with delayed bandit feedback. By separating the cost of delayed feedback
  from that of bandit feedback, our analysis allows us to obtain new results in three
  important settings. On the one hand, we derive the first optimal (up to logarithmic
  factors) regret bounds for combinatorial semi-bandits with delay and adversarial
  Markov decision processes with delay (and known transition functions).   On the
  other hand, we use our analysis to derive an efficient algorithm for linear bandits
  with delay achieving near-optimal regret bounds. Our novel regret decomposition
  shows that FTRL remains stable across multiple rounds under mild assumptions on
  the Hessian of the regularizer.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hoeven23a
month: 0
tex_title: A Unified Analysis of Nonstochastic Delayed Feedback for Combinatorial
  Semi-Bandits, Linear Bandits, and MDPs
firstpage: 1
lastpage: 37
page: 1-37
order: 1
cycles: false
bibtex_author: van der Hoeven, Dirk and Zierahn, Lukas and Lancewicki, Tal and Rosenberg,
  Aviv and Cesa-Bianchi, Nicol{\`o}
author:
- given: Dirk
  family: Hoeven
  prefix: van der
- given: Lukas
  family: Zierahn
- given: Tal
  family: Lancewicki
- given: Aviv
  family: Rosenberg
- given: Nicol√≤
  family: Cesa-Bianchi
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/hoeven23a/hoeven23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
