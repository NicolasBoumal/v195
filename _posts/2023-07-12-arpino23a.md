---
title: Statistical-Computational Tradeoffs in Mixed Sparse Linear Regression
abstract: We consider the problem of mixed sparse linear regression with two components,
  where two k-sparse signals β_1, β_2 ∈ R^p are to be recovered from n unlabelled
  noisy linear measurements. The sparsity is allowed to be sublinear in the dimension
  (k = o(p)), and the additive noise is assumed to be independent Gaussian with variance
  σ^2. Prior work has shown that the problem suffers from a k/SNR^2 -to- k^2/SNR^2
  statistical-to-computational gap, resembling other computationally challenging high-
  dimensional inference problems such as Sparse PCA and Robust Sparse Mean Estimation
  (Brennan and Bresler, 2020b); here SNR := ∥β1∥^2/σ^2 = ∥β2∥^2/σ^2 is the signal-to-noise
  ratio. We establish the existence of a more extensive k/SNR^2 -to- k^2 (SNR+1)^2/SNR^2
  computational barrier for this problem through the method of low-degree polynomials,
  but show that the problem is computationally hard only in a very narrow symmetric
  parameter regime. We identify a smooth information-computation tradeoff between
  the sample complexity n and runtime exp( Θ(k^2(SNR + 1)^2/(nSNR^2))) for any randomized
  algorithm in this hard regime. Via a simple reduction, this provides novel rigorous
  evidence for the existence of a computational barrier to solving exact support recovery
  in sparse phase retrieval with sample complexity n = o(k^2). Our second contribution
  is to analyze a simple thresholding algorithm which, outside of the narrow regime
  where the problem is hard, solves the associated mixed regression detection problem
  in O(np) time and matches the sample complexity required for (non-mixed) sparse
  linear regression of k(SNR+1)/SNR log p; this allows the recovery problem to be
  subsequently solved by state-of-the-art techniques from the dense case. As a special
  case of our results, we show that this simple algorithm is order-optimal among a
  large family of algorithms in solving exact signed support recovery in sparse linear
  regression. To the best of our knowledge, this is the first thorough study of the
  interplay between mixture symmetry, signal sparsity, and their joint impact on the
  computational hardness of mixed sparse linear regression.
section: Original Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: arpino23a
month: 0
tex_title: Statistical-Computational Tradeoffs in Mixed Sparse Linear Regression
firstpage: 921
lastpage: 986
page: 921-986
order: 921
cycles: false
bibtex_author: Arpino, Gabriel and Venkataramanan, Ramji
author:
- given: Gabriel
  family: Arpino
- given: Ramji
  family: Venkataramanan
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/arpino23a/arpino23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
