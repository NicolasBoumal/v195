---
title: Learning Narrow One-Hidden-Layer ReLU Networks
abstract: We consider the well-studied problem of learning a linear combination of
  $k$ ReLU activations with respect to a Gaussian distribution on inputs in $d$ dimensions.  We
  give the first polynomial-time algorithm that succeeds whenever $k$ is a constant.  All
  prior polynomial-time learners require additional assumptions on the network, such
  as positive combining coefficients or the matrix of hidden weight vectors being
  well-conditioned.Our approach is based on analyzing random contractions of higher-order
  moment tensors.  We use a multi-scale clustering procedure to argue that sufficiently
  close neurons can be collapsed together, sidestepping the conditioning issues present
  in prior work. This allows us to design an iterative procedure to discover individual
  neurons.
section: Original Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: chen23a
month: 0
tex_title: Learning Narrow One-Hidden-Layer ReLU Networks
firstpage: 5580
lastpage: 5614
page: 5580-5614
order: 5580
cycles: false
bibtex_author: Chen, Sitan and Dou, Zehao and Goel, Surbhi and Klivans, Adam and Meka,
  Raghu
author:
- given: Sitan
  family: Chen
- given: Zehao
  family: Dou
- given: Surbhi
  family: Goel
- given: Adam
  family: Klivans
- given: Raghu
  family: Meka
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/chen23a/chen23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
