---
title: Limits of Model Selection under Transfer Learning
abstract: 'Theoretical studies on \emph{transfer learning} (or \emph{domain adaptation})
  have so far focused on situations with a known hypothesis class or \emph{model};
  however in practice, some amount of model selection is usually involved, often appearing
  under the umbrella term or \emph{hyperparameter-tuning}: for example, one may think
  of the problem of \emph{tuning} for the right neural network architecture towards
  a target task, while leveraging data from a related \emph{source} task.  Now, in
  addition to the usual tradeoffs on approximation vs estimation errors involved in
  model selection, this problem brings in a new complexity term, namely, the \emph{transfer
  distance} between source and target distributions, which is known to vary with the
  choice of hypothesis class. We present a first study of this problem, focusing on
  classification; in particular, the analysis reveals some remarkable phenomena: \emph{adaptive
  rates}, i.e., those achievable with no distributional information, can be arbitrarily
  slower than \emph{oracle rates}, i.e., when given knowledge on \emph{distances}'
section: Original Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hanneke23c
month: 0
tex_title: Limits of Model Selection under Transfer Learning
firstpage: 5781
lastpage: 5812
page: 5781-5812
order: 5781
cycles: false
bibtex_author: Hanneke, Steve and Kpotufe, Samory and Mahdaviyeh, Yasaman
author:
- given: Steve
  family: Hanneke
- given: Samory
  family: Kpotufe
- given: Yasaman
  family: Mahdaviyeh
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/hanneke23c/hanneke23c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
