---
title: Lower Bounds for the Convergence of Tensor Power Iteration on Random Overcomplete
  Models
abstract: Tensor decomposition serves as a powerful primitive in statistics and machine
  learning, and has numerous applications in problems such as learning latent variable
  models or mixture of Gaussians. In this paper, we focus on using power iteration
  to decompose an overcomplete random tensor. Past work studying the properties of
  tensor power iteration either requires a non-trivial data-independent initialization,
  or is restricted to the undercomplete regime. Moreover, several papers implicitly
  suggest that logarithmically many iterations (in terms of the input dimension) are
  sufÔ¨Åcient for the power method to recover one of the tensor components.Here we present
  a novel analysis of the dynamics of tensor power iteration from random initialization
  in the overcomplete regime, where the tensor rank is much greater than its dimension.
  Surprisingly, we show that polynomially many steps are necessary for convergence
  of tensor power iteration to any of the true component, which refutes the previous
  conjecture. On the other hand, our numerical experiments suggest that tensor power
  iteration successfully recovers tensor components for a broad range of parameters
  in polynomial time. To further complement our empirical evidence, we prove that
  a popular objective function for tensor decomposition is strictly increasing along
  the power iteration path.Our proof is based on the Gaussian conditioning technique,
  which has been applied to analyze the approximate message passing (AMP) algorithm.
  The major ingredient of our argument is a conditioning lemma that allows us to generalize
  AMP-type analysis to non-proportional limit and polynomially many iterations of
  the power method.
section: Original Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wu23b
month: 0
tex_title: Lower Bounds for the Convergence of Tensor Power Iteration on Random Overcomplete
  Models
firstpage: 3783
lastpage: 3820
page: 3783-3820
order: 3783
cycles: false
bibtex_author: Wu, Yuchen and Zhou, Kangjie
author:
- given: Yuchen
  family: Wu
- given: Kangjie
  family: Zhou
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/wu23b/wu23b.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
