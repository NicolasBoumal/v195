---
title: 'Breaking the Lower Bound with (Little) Structure: Acceleration in Non-Convex
  Stochastic Optimization with Heavy-Tailed Noise'
abstract: In this paper, we consider the stochastic optimization problem with smooth
  but not necessarily convex objectives in the heavy-tailed noise regime, where the
  stochastic gradientâ€™s noise is assumed to have bounded $p$th moment ($p\in(1,2]$).
  This is motivated by a recent plethora of studies in the machine learning literature,
  which point out that, in comparison to the standard finite-variance assumption,
  the heavy-tailed noise regime is more appropriate for modern machine learning tasks
  such as training neural networks. In the heavy-tailed noise regime, Zhang et al.
  (2020) is the first to prove the $\Omega(T^{\frac{1-p}{3p-2}})$ lower bound for
  convergence (in expectation) and provides a simple clipping algorithm that matches
  this optimal rate. Later, Cutkosky and Mehta (2021) proposes another algorithm,
  which is shown to achieve the nearly optimal high-probability convergence guarantee
  $O(\log(T/\delta)T^{\frac{1-p}{3p-2}})$, where $\delta$ is the probability of failure.
  However, this desirable guarantee is only established under the additional assumption
  that the stochastic gradient itself is bounded in $p$th moment, which fails to hold
  even for quadratic objectives and centered Gaussian noise. In this work, we first
  improve the analysis of the algorithm in Later, Cutkosky and Mehta (2021) to obtain
  the same nearly optimal high-probability convergence rate $O(\log(T/\delta)T^{\frac{1-p}{3p-2}})$,
  without the above-mentioned restrictive assumption. Next, and curiously, we show
  that one can achieve a faster rate than that dictated by the lower bound $\Omega(T^{\frac{1-p}{3p-2}})$
  with only a tiny bit of structure, i.e., when the objective function $F(x)$ is assumed
  to be in the form of $\E_{\Xi\sim\domxi}[f(x,\Xi)]$, arguably the most widely applicable
  class of stochastic optimization problems. For this class of problems, we propose
  the first variance-reduced accelerated algorithm and establish that it guarantees
  a high-probability convergence rate of $O(\log(T/\delta)T^{\frac{1-p}{2p-1}})$ under
  a mild condition, which is faster than $\Omega(T^{\frac{1-p}{3p-2}})$. Notably,
  even when specialized to the standard finite-variance case ($p =2$), our result
  yields the (near-)optimal high-probability rate $O(\log(T/\delta)T^{-1/3})$, which
  is unknown before.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: liu23c
month: 0
tex_title: 'Breaking the Lower Bound with (Little) Structure: Acceleration in Non-Convex
  Stochastic Optimization with Heavy-Tailed Noise'
firstpage: 1
lastpage: 25
page: 1-25
order: 1
cycles: false
bibtex_author: Liu, Zijian and Zhang, Jiawei and Zhou, Zhengyuan
author:
- given: Zijian
  family: Liu
- given: Jiawei
  family: Zhang
- given: Zhengyuan
  family: Zhou
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/liu23c/liu23c.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
