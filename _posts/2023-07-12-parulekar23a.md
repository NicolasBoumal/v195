---
title: InfoNCE Loss Provably Learns Cluster-Preserving Representations
abstract: The goal of contrasting learning is to learn a representation that preserves
  underlying clusters by keeping samples with similar content, e.g. the “dogness”
  of a dog, close to each other in the space generated by the representation. A common
  and successful approach for tackling this unsupervised learning problem is minimizing
  the InfoNCE loss associated with the training samples, where each sample is associated
  with their augmentations (positive samples such as rotation, crop) and a batch of
  negative samples (unrelated samples). To the best of our knowledge, it was unanswered
  if the representation learned by minimizing the InfoNCE loss preserves the underlying
  data clusters, as it only promotes learning a representation that is faithful to
  augmentations, i.e., an image and its augmentations have the same representation.
  Our main result is to show that the representation learned by InfoNCE with a finite
  number of negative samples is also consistent with respect to {\em clusters} in
  the data, under the condition that the augmentation sets within clusters may be
  non-overlapping but are close and intertwined, relative to the complexity of the
  learning function class.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: parulekar23a
month: 0
tex_title: InfoNCE Loss Provably Learns Cluster-Preserving Representations
firstpage: 1914
lastpage: 1961
page: 1914-1961
order: 1914
cycles: false
bibtex_author: Parulekar, Advait and Collins, Liam and Shanmugam, Karthikeyan and
  Mokhtari, Aryan and Shakkottai, Sanjay
author:
- given: Advait
  family: Parulekar
- given: Liam
  family: Collins
- given: Karthikeyan
  family: Shanmugam
- given: Aryan
  family: Mokhtari
- given: Sanjay
  family: Shakkottai
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/parulekar23a/parulekar23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
