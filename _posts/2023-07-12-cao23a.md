---
title: The Implicit Bias of Batch Normalization in Linear Models and Two-layer Linear
  Convolutional Neural Networks
abstract: We study the implicit bias of batch normalization trained by gradient descent.
  We show that when learning a linear model with batch normalization for binary classification,
  gradient descent converges to a uniform margin classifier on the training data with
  an $\exp(-\Omega(\log^2t))$ convergence rate. This distinguishes linear models with
  batch normalization from those without batch normalization in terms of both the
  type of implicit bias and the convergence rate. We then further extend our result
  to a class of two-layer, single-filter convolutional neural networks, and show that
  batch normalization has an implicit bias towards a patch-wise uniform margin. Based
  on two examples, we demonstrate that patch-wise uniform margin classifiers can outperform
  the maximum margin classifiers in certain learning problems. Our results contribute
  to a better theoretical understanding of batch normalization.
section: Original Papers
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: cao23a
month: 0
tex_title: The Implicit Bias of Batch Normalization in Linear Models and Two-layer
  Linear Convolutional Neural Networks
firstpage: 5699
lastpage: 5753
page: 5699-5753
order: 5699
cycles: false
bibtex_author: Cao, Yuan and Zou, Difan and Li, Yuanzhi and Gu, Quanquan
author:
- given: Yuan
  family: Cao
- given: Difan
  family: Zou
- given: Yuanzhi
  family: Li
- given: Quanquan
  family: Gu
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/cao23a/cao23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
