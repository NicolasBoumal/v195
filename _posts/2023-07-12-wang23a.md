---
title: 'Convergence of AdaGrad for Non-convex Objectives: Simple Proofs and Relaxed
  Assumptions'
abstract: We provide a simple convergence proof for AdaGrad optimizing non-convex
  objectives under only affine noise variance and bounded smoothness assumptions.
  The proof is essentially based on a novel auxiliary function $\xi$ that helps eliminate
  the complexity of handling the correlation between the numerator and denominator
  of AdaGradâ€™s update. Leveraging simple proofs, we are able to obtain tighter results
  than existing results \citep{faw2022power} and extend the analysis to several new
  and important cases. Specifically, for the over-parameterized regime, we show that
  AdaGrad needs only $\mathcal{O}(\frac{1}{\varepsilon^2})$ iterations to ensure the
  gradient norm smaller than $\varepsilon$, which matches the rate of SGD and significantly
  tighter than existing rates $\mathcal{O}(\frac{1}{\varepsilon^4})$ for AdaGrad.
  We then discard the bounded smoothness assumption, and consider a realistic assumption
  on smoothness called $(L_0,L_1)$-smooth condition, which allows local smoothness
  to grow with the gradient norm. Again based on the auxiliary function $\xi$, we
  prove that AdaGrad succeeds in converging under $(L_0,L_1)$-smooth condition as
  long as the learning rate is lower than a threshold. Interestingly, we further show
  that the requirement on learning rate under the $(L_0,L_1)$-smooth condition is
  necessary via proof by contradiction, in contrast with the case of uniform smoothness
  conditions where convergence is guaranteed regardless of learning rate choices.
  Together, our analyses broaden the understanding of AdaGrad and demonstrate the
  power of the new auxiliary function in the investigations of AdaGrad.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: wang23a
month: 0
tex_title: 'Convergence of AdaGrad for Non-convex Objectives: Simple Proofs and Relaxed
  Assumptions'
firstpage: 161
lastpage: 190
page: 161-190
order: 161
cycles: false
bibtex_author: Wang, Bohan and Zhang, Huishuai and Ma, Zhiming and Chen, Wei
author:
- given: Bohan
  family: Wang
- given: Huishuai
  family: Zhang
- given: Zhiming
  family: Ma
- given: Wei
  family: Chen
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/wang23a/wang23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
