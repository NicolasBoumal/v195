---
title: 'Variance-Dependent Regret Bounds for Linear Bandits and Reinforcement Learning:
  Adaptivity and Computational Efficiency'
abstract: Recently, several studies \citep{zhou2021nearly, zhang2021variance, kim2021improved,
  zhou2022computationally} have provided variance-dependent regret bounds for linear
  contextual bandits, which interpolates the regret for the worst-case regime and
  the deterministic reward regime. However, these algorithms are either computationally
  intractable or unable to handle unknown variance of the noise. In this paper, we
  present a novel solution to this open problem by proposing the \emph{first computationally
  efficient} algorithm for linear bandits with heteroscedastic noise. Our algorithm
  is adaptive to the unknown variance of noise and achieves an $\tilde{O}(d \sqrt{\sum_{k
  = 1}^K \sigma_k^2} + d)$  regret, where $\sigma_k^2$ is the \emph{variance} of the
  noise at the round $k$, $d$ is the dimension of the contexts and $K$ is the total
  number of rounds. Our results are based on an adaptive variance-aware confidence
  set enabled by a new Freedman-type concentration inequality for self-normalized
  martingales and a multi-layer structure to stratify the context vectors into different
  layers with different uniform upper bounds on the uncertainty.    Furthermore, our
  approach can be extended to linear mixture Markov decision processes (MDPs) in reinforcement
  learning. We propose a variance-adaptive algorithm for linear mixture MDPs, which
  achieves a problem-dependent horizon-free regret bound that can gracefully reduce
  to a nearly constant regret for deterministic MDPs. Unlike existing nearly minimax
  optimal algorithms for linear mixture MDPs, our algorithm does not require explicit
  variance estimation of the transitional probabilities or the use of high-order moment
  estimators to attain horizon-free regret. We believe the techniques developed in
  this paper can have independent value for general online decision making problems.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: zhao23a
month: 0
tex_title: 'Variance-Dependent Regret Bounds for Linear Bandits and Reinforcement
  Learning: Adaptivity and Computational Efficiency'
firstpage: 1
lastpage: 44
page: 1-44
order: 1
cycles: false
bibtex_author: Zhao, Heyang and He, Jiafan and Zhou, Dongruo and Zhang, Tong and Gu,
  Quanquan
author:
- given: Heyang
  family: Zhao
- given: Jiafan
  family: He
- given: Dongruo
  family: Zhou
- given: Tong
  family: Zhang
- given: Quanquan
  family: Gu
date: 2023-07-12
address: 
container-title: Proceedings of Thirty Sixth Conference on Learning Theory
volume: '195'
genre: inproceedings
issued:
  date-parts:
  - 2023
  - 7
  - 12
pdf: https://proceedings.mlr.press/v195/zhao23a/zhao23a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
